---
title: 4. Hive
description: OLake Apache Iceberg writer description
sidebar_position: 4
---

## Hive Catalog

:::info
Use `iceberg_s3_path` with `s3a` prefix if your Hive is configured so. This will work for most use cases. Otherwise, use `iceberg_path` with `s3` prefix.
:::


<Tabs>

<TabItem value="hive-ui" label="OLake UI" default>

<HiveIcebergWriterUIConfigDetails/>

</TabItem>

<TabItem value="hive-cli" label="OLake CLI" default>

<HiveIcebergWriterConfig/>

### Hive Configuration Parameters

<HiveIcebergWriterConfigDetails/>


</TabItem>

</Tabs>


You can query the data via:

<CatalogQuery/>

For S3 related permissions which is needed to write data to S3, refer to the [AWS S3 Permissions](../../parquet/partitioning) documentation.

### Using GCP Dataproc Metastore (Hive) with Google Cloud Storage (GCS)

OLake supports using Google Cloud Dataproc Metastore (Hive) as the Iceberg catalog and Google Cloud Storage (GCS) as the data lake destination. This allows you to leverage GCP-native services for scalable, managed metadata and storage.

**Dataproc Metastore**
![dataproc-metastore](/img/docs/iceberg/hive-gcp-dataproc.png)

#### Step-by-Step Setup

1. **Create a GCP Project** (if you don't have one).
2. **Provision a Dataproc Metastore (Hive):**
   - Go to the GCP Console → Dataproc → Metastore services.
   - Click "Create Metastore Service".
   - Fill in service name, location, version, release channel, port (default: 9083), and service tier.
   - Set the endpoint protocol to **Thrift**.
   - Expose the service to your running network (VPC/subnet).
   - Enable the Data Catalog sync option if desired.
   - Choose database type and other options as needed.
   - Click Submit. Creation may take 20–30 minutes.
3. **Expose the Metastore endpoint** to the network where OLake will run (ensure network connectivity and firewall rules allow access to the Thrift port).
4. **Create or choose a GCS bucket** for Iceberg data.
5. **Deploy OLake** in the same network (or with access to the Metastore endpoint).

#### GCP-Hive OLake Destination Config

```json title="destination.json (GCP Hive + GCS)"
{
  "type": "ICEBERG",
  "writer": {
    "catalog_type": "hive",
    "hive_uri": "thrift://<METASTORE_IP>:9083",
    "hive_clients": 10,
    "hive_sasl_enabled": false,
    "iceberg_db": "olake_iceberg",
    "iceberg_s3_path": "gs://<hive-dataproc-generated-bucket>/hive-warehouse",
    "aws_region": "us-central1"
  }
}
```

- **Replace `<METASTORE_IP>`** with your Dataproc Metastore’s internal IP or hostname.
- Replace `<hive-dataproc-generated-bucket>` with the Dataproc Metastore generated `hive.metastore.warehouse.dir` bucket.
- **Set `aws_region`** to your GCP region (e.g., `us-central1`).

#### Notes

- The `hive_uri` must use the Thrift protocol and point to your Dataproc Metastore endpoint.
- The `iceberg_s3_path` can use the `gs://` prefix for GCS buckets.
- Ensure OLake has network access to the Metastore and permissions to write to the GCS bucket.
- Data written will be in Iceberg format, queryable via compatible engines (e.g., Spark, Trino) configured with the same Hive Metastore and GCS bucket.

#### Troubleshooting

- If you encounter connection issues, verify firewall rules and VPC peering between OLake and the Dataproc Metastore.
- Ensure the Dataproc Metastore is in a running state and the Thrift port is open.

:::info
If you wish to test out the REST Catalog locally, you can use the [docker-compose](../docker-compose) setup. The local test setup uses Minio as an S3-compatible storage and other all [supported catalog types](../docker-compose#local-catalog--test-setup). 

You can then setup local spark to run queries on the iceberg tables created in the local test setup.
:::